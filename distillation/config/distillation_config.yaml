# Knowledge Distillation Configuration
# Whisper Large (Teacher) -> Whisper Base (Student)

project:
  name: "whisper_base_vietnamese_distillation"
  description: "Knowledge distillation from PhoWhisper-large to OpenAI Whisper-base for Vietnamese ASR"
  language: "vi"
  
experiment_name: "whisper_base_vietnamese_distillation"  # Used for metrics logging
  
paths:
  # Dataset paths - CHANGE TO YOUR PATHS
  phowhisper_audio: "path/to/phowhisper/audio/"
  phowhisper_transcripts: "path/to/phowhisper/transcripts/"
  
  # Dataset path (PhoAudioBook downloaded)
  preprocessed_dataset: "./preprocessing_data/phoaudiobook_100h/"
  
  # Teacher logits storage (will store soft labels from teacher)
  teacher_logits_dir: "./data/teacher_logits/"
  
  # Pretrained weights paths
  teacher_weights: "../model/pretrained_weights/whisper_large_tf.weights.h5"
  student_weights: "../model/pretrained_weights/whisper_small_tf.weights.h5"
  
  # Output paths
  checkpoints_dir: "./checkpoints/"
  logs_dir: "./logs/"
  final_model_dir: "./final_model/"

teacher:
  model_name: "vinai/PhoWhisper-large"  # PhoWhisper for Vietnamese
  temperature: 2.0  # FIXED: Must match distillation.temperature
  batch_size: 8  # Teacher inference batch size
  use_pytorch: true  # PyTorch is faster for inference
  save_logits_format: "npy"  # Use 'h5' for larger datasets
  
student:
  model_name: "base"  # OpenAI Whisper base (74M params)
  pretrained: true  # Load pretrained weights
  load_from_openai: true  # Load OpenAI pretrained weights
  freeze_encoder_initially: false  # Set true to train decoder first

distillation:
  # Mini-batch mode (saves disk space)
  enable_mini_batch: true  # Enable mini-batch distillation pipeline
  mini_batch_size: 100  # Samples per mini-batch
  auto_cleanup_logits: true  # Auto-delete logits after training
  one_shot_mode: true  # Each sample trained only once
  
  # Loss weights (must sum to 1.0)
  soft_loss_weight: 0.7  # KL divergence with teacher (soft targets)
  hard_loss_weight: 0.3  # Cross entropy with ground truth (hard labels)
  feature_loss_weight: 0.0  # Intermediate layer matching (optional, disabled)
  
  # Temperature for distillation
  temperature: 2.0  # Balanced temp for stable training (5.0 was too high, caused flat loss)
  
  # Training hyperparameters
  epochs: 1  # One-shot distillation
  batch_size: 1  # Must use 1 to fit in 8GB VRAM (both 2 and 4 caused OOM)
  gradient_accumulation_steps: 8  # Maintain effective_batch_size=8
  effective_batch_size: 8  # 1 Ã— 8 = 8 (unchanged)  
  
  # Optimizer configuration
  optimizer: "adamw"  # AdamW optimizer with weight decay
  learning_rate: 0.00005  # FIXED: 5e-5 was parsed as string, now explicit float
  weight_decay: 0.01  # Weight decay for regularization
  beta_1: 0.9  # Adam beta1 parameter
  beta_2: 0.98  # Adam beta2 parameter
  epsilon: 0.000001  # Epsilon for numerical stability
  
  # Learning rate schedule
  lr_schedule: "cosine"  # FIXED: Cosine decay for better convergence
  warmup_steps: 500  # FIXED: Gradual warmup to prevent early instability
  min_learning_rate: 0.000001  # Minimum LR at end of cosine schedule
  
  # Mixed precision training (CRITICAL for 8GB VRAM)
  mixed_precision: true  # Enable mixed precision (fp16)
  policy: "mixed_float16"  # Use fp16 for forward/backward, fp32 for weights
  
  # Gradient clipping
  max_gradient_norm: 1.0  # Clip gradients to prevent explosion
  
  # Checkpointing
  save_every_n_steps: 1000  # Save less frequently (not needed with epochs=1)
  keep_last_n_checkpoints: 3  # Keep only 3 recent checkpoints (save disk space)
  save_best_only: false  # Save all checkpoints (no validation to determine "best")
  
  # Validation & Evaluation (disabled for one-shot training)
  eval_every_n_steps: 10000  # Effectively disabled (batch has ~100 steps)
  eval_samples: 100  # Number of samples for quick validation
  
  # Early stopping (disabled for one-shot training)
  early_stopping_patience: 100  # Effectively disabled
  early_stopping_metric: "wer"  # Word Error Rate
  early_stopping_mode: "min"  # Lower is better

hardware:
  device: "GPU"  # Use GPU for training
  gpu_id: 0  # GPU device ID
  gpu_memory_limit: 8192  # MB (8GB for RTX 5060)
  allow_growth: true  # Allow TensorFlow to allocate memory as needed
  
  # Data loading
  num_workers: 4  # Parallel data loading workers
  prefetch_size: 2  # Prefetch batches
  pin_memory: true  # Pin memory for faster GPU transfer

data:
  # Dataset filtering - IMPORTANT
  use_full_dataset: false  # Set true to use full 1000h dataset
  max_hours: 100  # Limit to 100 hours of audio
  max_samples: null  # Limit by number of samples (null = use max_hours)
  train_split: 0.95  # 95% train, 5% validation
  random_seed: 42  # For reproducibility
  
  # Audio processing
  sample_rate: 16000  # 16kHz audio
  n_mels: 80  # Number of mel filterbanks
  n_fft: 400  # FFT window size
  hop_length: 160  # Hop length for STFT
  max_audio_length: 12.0  # Maximum audio length in seconds (base model)
  max_text_length: 448  # Maximum text sequence length (tokens)
  language: "vi"  # Vietnamese language code
  task: "transcribe"  # Transcription task
  
  # Timestamp settings
  return_timestamps: false  # Disable timestamps - standard transcription only
  timestamp_granularity: "word"  # Not used when return_timestamps=false
  
  # Data augmentation
  use_augmentation: true  # Enable data augmentation
  spec_augment: true  # SpecAugment for mel spectrograms
  time_masking: true  # Time axis masking
  freq_masking: true  # Frequency axis masking

logging:
  # Console logging
  log_level: "INFO"  # Logging level
  log_every_n_steps: 10  # Print to console every N steps
  
  # Step-level metrics logging
  log_step_metrics: true  # Enable per-step metrics logging (not just per-batch)
  save_step_metrics_every: 1  # Save step metrics every N steps (1=every step, 5=every 5 steps)
  
  # TensorBoard
  use_tensorboard: true  # Enable TensorBoard logging
  tensorboard_dir: "./logs/tensorboard/"  # TensorBoard log directory
  
  # Weights & Biases (optional)
  use_wandb: false  # Enable W&B logging
  wandb_project: "whisper-distillation"  # W&B project name
  wandb_entity: null  # W&B entity (organization)

# Advanced options
advanced:
  # Activation checkpointing to save VRAM
  use_gradient_checkpointing: true  # Enabled to prevent OOM with batch_size=2
  
  # Progressive unfreezing (not used with epochs=1)
  progressive_unfreezing: false  # Gradually unfreeze layers
  unfreeze_schedule: [0, 3, 6]  # Epochs at which to unfreeze layers
  
  # Temperature annealing (not used with epochs=1)
  temperature_annealing: false  # Gradually reduce temperature
  temperature_schedule: [5.0, 3.0, 1.0]  # Temperature values per epoch
