# Knowledge Distillation Pipeline for Whisper Vietnamese

**Docker + PyTorch + RTX 5060 Setup**

Distillation from **Whisper Large-v2** (teacher) â†’ **Whisper Small** (student) on PhoWhisper dataset.

---

## ğŸ¯ Project Goal

Train Whisper Small model achieving **85-90%** of Whisper Large performance using knowledge distillation.

**Target Metrics (100h dataset):**
- WER: 8.5-10% (vs Large: ~6.5%)
- Model size: 244M params (vs Large: 1550M)
- Speed: 4-5x faster inference
- Hardware: RTX 5060 8GB with Docker

---

## ğŸ–¥ï¸ System Requirements

### Hardware
- GPU: RTX 5060 8GB (hoáº·c GPU tÆ°Æ¡ng Ä‘Æ°Æ¡ng)
- RAM: 16GB+
- Storage: 60GB+ free space

### Software
- Windows 11 (hoáº·c Windows 10 21H2+)
- WSL2 (Ubuntu 22.04 recommended)
- Docker Desktop for Windows
- NVIDIA Driver: 576.88+

---

# ğŸ“– HÆ¯á»šNG DáºªN SETUP HOÃ€N CHá»ˆNH

## PHáº¦N 1: SETUP WSL2 + DOCKER

### BÆ°á»›c 1: CÃ i Ä‘áº·t WSL2 (trÃªn Windows PowerShell admin)

```powershell
# Enable WSL
wsl --install

# Hoáº·c náº¿u Ä‘Ã£ cÃ³ WSL1, upgrade lÃªn WSL2
wsl --set-default-version 2

# CÃ i Ubuntu 22.04
wsl --install -d Ubuntu-22.04
```

**Restart mÃ¡y sau khi cÃ i!**

---

### BÆ°á»›c 2: CÃ i Docker Desktop for Windows

1. Download Docker Desktop tá»«: https://www.docker.com/products/docker-desktop/
2. CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng Docker Desktop
3. Trong Docker Desktop Settings:
   - VÃ o **Resources** â†’ **WSL Integration**
   - Enable cho Ubuntu-22.04
   - Click **Apply & Restart**

---

### BÆ°á»›c 3: Verify Docker vÃ  GPU trong WSL

Má»Ÿ WSL terminal:

```bash
# Check Docker
docker --version
docker ps

# Check GPU accessible
nvidia-smi
```

**Expected:**
```
Docker version 28.x.x
NVIDIA GeForce RTX 5060
```

---

### BÆ°á»›c 4: Test NVIDIA Container Toolkit

```bash
# Test GPU in Docker
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

**Expected:** Tháº¥y RTX 5060 info

---

## PHáº¦N 2: SETUP DOCKER CONTAINER

### BÆ°á»›c 5: Pull NVIDIA TensorFlow Image

```bash
# Navigate to project
cd /mnt/c/Users/Admin/Desktop/dat301m/Speech_to_text/distillation

# Pull image (chá»©a PyTorch + TensorFlow + CUDA)
docker pull nvcr.io/nvidia/tensorflow:25.02-tf2-py3
```

**Thá»i gian:** ~5-10 phÃºt (táº£i ~10GB)

---

### BÆ°á»›c 6: Build Custom Docker Image

```bash
# Build image vá»›i dependencies
chmod +x run_docker.sh
./run_docker.sh
```

**Image bao gá»“m:**
- PyTorch 2.9.0 + CUDA 12.8 (há»— trá»£ RTX 5060)
- Transformers + Whisper models
- librosa, soundfile, jiwer
- TensorBoard, pandas, numpy

---

### BÆ°á»›c 7: Test GPU trong Container

```bash
# Test script
./test_docker_gpu.sh
```

**Expected output:**
```
GPUs found: 1
PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
âœ“ Transformers imported successfully
```

---

## PHáº¦N 3: TEST TEACHER MODEL

### BÆ°á»›c 8: Access Container vÃ  Test

```bash
# VÃ o container
docker-compose exec whisper-distillation bash

# Trong container, test PyTorch teacher
python teacher/load_teacher_pytorch.py
```

**Expected:**
```
Loading Whisper Teacher Model (PyTorch)
Model: openai/whisper-large-v2
Device: cuda
GPU: NVIDIA GeForce RTX 5060
âœ“ Model loaded on cuda
âœ“ Forward pass successful!
ALL TESTS PASSED!
```

---

### BÆ°á»›c 9: Configure Pipeline

Edit `config/distillation_config.yaml`:

```yaml
paths:
  preprocessed_dataset: "../preprocessing_data/processed_dataset/"
  teacher_logits_dir: "./data/teacher_logits/"
  checkpoints_dir: "./checkpoints/"

data:
  use_full_dataset: false
  max_hours: 100           # 100 giá» data
  train_split: 0.95

distillation:
  epochs: 20               # TÄƒng vÃ¬ Ã­t data
  batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-5
  mixed_precision: true
```

---

## PHáº¦N 4: CHáº Y DISTILLATION PIPELINE

### Pipeline Overview

```
Step 1: Generate Teacher Logits (6-12 giá» vá»›i 100h data)
   â†“
Step 2: Train Student (1-2 tuáº§n)
   â†“
Step 3: Evaluate (1 ngÃ y)
```

---

### Step 1: Generate Teacher Logits

```bash
# Trong Docker container
python scripts/step1_generate_teacher_logits.py
```

**Output:**
- `data/teacher_logits/` chá»©a soft labels (~30-35GB cho 100h)
- Thá»i gian: 6-12 giá» vá»›i RTX 5060

---

### Step 2: Train Student

```bash
# Trong Docker container
python scripts/step2_train_distillation.py
```

**Monitoring:**
```bash
# Tá»« WSL (ngoÃ i container)
docker-compose exec whisper-distillation tensorboard --logdir=/workspace/distillation/logs/tensorboard/ --host=0.0.0.0
```

Access: http://localhost:6006

---

### Step 3: Evaluate

```bash
# Trong Docker container
python scripts/step3_evaluate_model.py
```

**Expected WER:** 8.5-10% trÃªn Vietnamese test set (100h data)

---

## ğŸ“ Project Structure

```
distillation/
â”œâ”€â”€ README.md                    # Báº¡n Ä‘ang Ä‘á»c file nÃ y
â”œâ”€â”€ Dockerfile                   # Docker image definition
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ run_docker.sh                # Build & start container
â”œâ”€â”€ test_docker_gpu.sh           # Test GPU setup
â”œâ”€â”€ config/
â”‚   â””â”€â”€ distillation_config.yaml # Cáº¥u hÃ¬nh chÃ­nh
â”‚
â”œâ”€â”€ teacher/                     # Teacher model (PyTorch)
â”‚   â”œâ”€â”€ load_teacher_pytorch.py # Load Whisper Large (MAIN)
â”‚   â”œâ”€â”€ load_teacher_tf.py      # TF version (deprecated)
â”‚   â”œâ”€â”€ verify_teacher.py       # Verify teacher
â”‚   â””â”€â”€ teacher_utils.py        # Utilities
â”‚
â”œâ”€â”€ student/                     # Student model
â”‚   â”œâ”€â”€ load_student.py
â”‚   â””â”€â”€ model_setup.py
â”‚
â”œâ”€â”€ data/                        # Data processing
â”‚   â””â”€â”€ teacher_logits/         # Generated soft labels
â”‚
â”œâ”€â”€ training/                    # Training components
â”‚   â”œâ”€â”€ distillation_trainer.py
â”‚   â””â”€â”€ loss_functions.py
â”‚
â”œâ”€â”€ evaluation/                  # Evaluation
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ scripts/                     # Main execution scripts
â”‚   â”œâ”€â”€ step1_generate_teacher_logits.py
â”‚   â”œâ”€â”€ step2_train_distillation.py
â”‚   â””â”€â”€ step3_evaluate_model.py
â”‚
â”œâ”€â”€ checkpoints/                 # Model checkpoints (gitignored)
â””â”€â”€ logs/                        # Training logs (gitignored)
```

---

## ğŸ”§ Configuration Details

### Dataset Configuration (100h)

```yaml
data:
  use_full_dataset: false
  max_hours: 100           # Giá»›i háº¡n 100 giá»
  max_samples: null
  train_split: 0.95        # 95h train, 5h validation
  random_seed: 42
```

### Distillation Hyperparameters

```yaml
distillation:
  soft_loss_weight: 0.7    # KL divergence vá»›i teacher
  hard_loss_weight: 0.3    # Cross entropy vá»›i ground truth
  temperature: 3.0         # Temperature scaling
  
  epochs: 20               # Nhiá»u epochs vÃ¬ Ã­t data
  batch_size: 2
  gradient_accumulation_steps: 4
  effective_batch_size: 8
  
  learning_rate: 5.0e-5
  warmup_steps: 200
  mixed_precision: true    # FP16 cho 8GB VRAM
```

---

## ğŸ“Š Expected Results

### Performance (100h dataset)

| Model | WER | Params | Inference Speed |
|-------|-----|--------|----------------|
| Teacher (Large) | 6.5% | 1550M | 1.0x |
| Student (Pretrained) | 11-12% | 244M | 5.2x |
| **Student (Distilled)** | **8.5-10%** | **244M** | **5.2x** |

**Improvement:** 2-3% WER reduction vs pretrained Small

---

### Timeline (RTX 5060 8GB)

```
Week 1:
  Day 1-2: Setup WSL + TensorFlow
  Day 3-4: Generate teacher logits (overnight)
  Day 5-7: Start training

Week 2-3:
  Main distillation training
  
Week 4:
  Fine-tuning + evaluation
  
Total: 3-4 weeks
```

---

## ğŸ’¡ Tips & Troubleshooting

### Memory Optimization

**OOM Error:**
```yaml
batch_size: 1
gradient_accumulation_steps: 8
```

### Training Tips

1. **Monitor GPU memory:**
   ```bash
   watch -n 1 nvidia-smi
   ```

2. **Save checkpoints frequently:**
   ```yaml
   save_every_n_steps: 500
   ```

3. **Enable gradient checkpointing náº¿u OOM:**
   ```yaml
   use_gradient_checkpointing: true
   ```

---

### Common Issues

**Issue 1: Docker khÃ´ng start**
```bash
# Check Docker Desktop Ä‘ang cháº¡y
# Enable WSL integration trong Docker Settings
```

**Issue 2: Container khÃ´ng tháº¥y GPU**
```bash
# Test GPU access
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi

# Náº¿u lá»—i: Update NVIDIA driver trÃªn Windows
```

**Issue 3: Out of memory trong container**
```yaml
# Trong load_teacher_pytorch.py, GPU memory Ä‘Ã£ set 6GB
# Náº¿u váº«n OOM: giáº£m batch_size trong config
batch_size: 1
```

**Issue 4: Container restart máº¥t data**
```bash
# Data Ä‘Æ°á»£c persist qua Docker volumes
# Check: docker volume ls
# Checkpoints vÃ  logs trong ./checkpoints/ vÃ  ./logs/
```

---

## ğŸš€ Quick Start Commands

### Setup Docker (one-time)

```bash
# Navigate to project
cd /mnt/c/Users/Admin/Desktop/dat301m/Speech_to_text/distillation

# Build and start container
./run_docker.sh

# Test GPU
./test_docker_gpu.sh
```

---

### Run Distillation Pipeline

```bash
# 1. Access container
docker-compose exec whisper-distillation bash

# 2. Test teacher model
python teacher/load_teacher_pytorch.py

# 3. Generate teacher logits (6-12 giá»)
python scripts/step1_generate_teacher_logits.py

# 4. Train student (1-2 tuáº§n)
python scripts/step2_train_distillation.py

# 5. Evaluate
python scripts/step3_evaluate_model.py
```

---

### Docker Management

```bash
# Start container
docker-compose up -d

# Stop container
docker-compose down

# View logs
docker-compose logs -f

# Restart
docker-compose restart
```

---

## ğŸ“š References

- [Whisper Paper](https://arxiv.org/abs/2212.04356)
- [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
- [Docker + NVIDIA GPU](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [PyTorch RTX 5060 Support](https://github.com/pytorch/pytorch/issues)
- [NVIDIA NGC Containers](https://catalog.ngc.nvidia.com/)

---

## ğŸ“§ Support

**Docker issues:** Check `./test_docker_gpu.sh` output  
**GPU issues:** Run `nvidia-smi` trong container  
**Training issues:** Check `logs/` folder  
**Performance:** Review `config/distillation_config.yaml`

---

## âš™ï¸ Technical Stack

- **Base Image:** `nvcr.io/nvidia/tensorflow:25.02-tf2-py3`
- **PyTorch:** 2.9.0 + CUDA 12.8
- **Transformers:** 4.57.1
- **Teacher Model:** `openai/whisper-large-v2` (PyTorch)
- **Student Model:** `openai/whisper-small` (TensorFlow - planned)
- **GPU:** NVIDIA RTX 5060 8GB
